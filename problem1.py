from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_extract
import os

def main():
    # -----------------------------------
    # åˆå§‹åŒ– SparkSessionï¼ˆåœ¨ master ä¸Šæ‰§è¡Œï¼‰
    # -----------------------------------
    spark = (
        SparkSession.builder
        .appName("Problem1_LogLevelDistribution")
        .master("local[*]")  # âœ… æœ¬åœ°è¯»æ–‡ä»¶ï¼Œå®‰å…¨å¯é 
        .getOrCreate()
    )

    # -----------------------------------
    # è·¯å¾„é…ç½®
    # -----------------------------------
    base_dir = "/home/ubuntu/fall-2025-a06-jc3482/data"
    input_dir = f"file://{base_dir}/raw/*/*.log"
    output_dir = f"{base_dir}/output"
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ“‚ Reading logs from: {input_dir}")

    # -----------------------------------
    # è¯»å–æ—¥å¿—æ–‡ä»¶
    # -----------------------------------
    logs = spark.read.text(input_dir)
    print(f"âœ… Total lines loaded: {logs.count():,}")

    # -----------------------------------
    # æå–æ—¥å¿—ç­‰çº§
    # -----------------------------------
    logs = logs.withColumn(
        "log_level",
        regexp_extract(col("value"), r"(INFO|WARN|ERROR|DEBUG)", 1)
    ).filter(col("log_level") != "")

    # -----------------------------------
    # ç»Ÿè®¡ log level åˆ†å¸ƒ
    # -----------------------------------
    counts = logs.groupBy("log_level").count().orderBy("count", ascending=False)

    counts.coalesce(1).write.csv(
        os.path.join(output_dir, "problem1_counts.csv"),
        header=True,
        mode="overwrite"
    )

    # -----------------------------------
    # éšæœºæŠ½æ · 10 æ¡æ—¥å¿—è¡Œ
    # -----------------------------------
    sample_logs = logs.sample(False, 0.001).limit(10)
    sample_logs.coalesce(1).write.csv(
        os.path.join(output_dir, "problem1_sample.csv"),
        header=True,
        mode="overwrite"
    )

    # -----------------------------------
    # ç”Ÿæˆ summary æ–‡ä»¶
    # -----------------------------------
    total = logs.count()
    summary_path = os.path.join(output_dir, "problem1_summary.txt")

    with open(summary_path, "w") as f:
        f.write(f"Total log lines analyzed: {total}\n\n")
        f.write("Log level distribution:\n")
        for row in counts.collect():
            f.write(f"{row['log_level']}: {row['count']}\n")

    print("\nâœ… Done! Outputs saved to:")
    print(f" - {output_dir}/problem1_counts.csv/")
    print(f" - {output_dir}/problem1_sample.csv/")
    print(f" - {summary_path}\n")

    spark.stop()

if __name__ == "__main__":
    main()
